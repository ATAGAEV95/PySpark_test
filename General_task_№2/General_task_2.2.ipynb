{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c19b03-498e-4dbb-9c2d-244151e08649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+------------+--------------------+--------------------+\n",
      "| id|                name|               email|                city|         age|              salary|   registration_date|\n",
      "+---+--------------------+--------------------+--------------------+------------+--------------------+--------------------+\n",
      "|  1|             ATAGAEV|78nbc2dff41nak647...|vbn345967nd34sf33...| eight8zero1|one1three1four4fo...|two1zero1one1seve...|\n",
      "|  2|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...| five5three1|one1two1eight8six...|two1zero1one1zero...|\n",
      "|  3|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|   six6nine9|four4two1six6one1...|two1zero1two1zero...|\n",
      "|  4|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|    six6two1|nine9two1four4eig...|one1nine9eight8tw...|\n",
      "|  5|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...| eight8five5|four4zero1two1eig...|two1zero1zero1two...|\n",
      "|  6|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|   two1four4|six6zero1zero1thr...|two1zero1two1zero...|\n",
      "|  7|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|   four4two1|nine9three1two1ni...|two1zero1zero1six...|\n",
      "|  8|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...| eight8zero1|one1one1nine9two1...|two1zero1zero1thr...|\n",
      "|  9|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...| seven7four4|three1two1one1fiv...|one1nine9seven7si...|\n",
      "| 10|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|seven7eight8|one1five5seven7fi...|two1zero1two1zero...|\n",
      "| 11|78nbc2dff41nak647...|                NULL|vbn345967nd34sf33...|  four4nine9|one1four4seven7ni...|two1zero1one1six6...|\n",
      "| 12|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|  six6three1|seven7one1nine9fi...|one1nine9nine9thr...|\n",
      "| 13|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|  nine9five5|one1zero1six6thre...|                NULL|\n",
      "| 14|                NULL|78nbc2dff41nak647...|vbn345967nd34sf33...|  six6three1|five5eight8four4e...|two1zero1two1thre...|\n",
      "| 15|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|  five5nine9|two1zero1three1si...|two1zero1zero1thr...|\n",
      "| 16|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...| three1five5|three1eight8three...|two1zero1one1zero...|\n",
      "| 17|78nbc2dff41nak647...|                NULL|vbn345967nd34sf33...|eight8three1|one1four4three1th...|one1nine9six6six6...|\n",
      "| 18|78nbc2dff41nak647...|                NULL|vbn345967nd34sf33...| seven7nine9|eight8six6four4tw...|one1nine9nine9nin...|\n",
      "| 19|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...|  three1two1|eight8two1nine9tw...|two1zero1one1eigh...|\n",
      "| 20|78nbc2dff41nak647...|78nbc2dff41nak647...|vbn345967nd34sf33...| five5seven7|one1three1nine9fo...|two1zero1zero1fiv...|\n",
      "+---+--------------------+--------------------+--------------------+------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, concat, lit, when, rand, current_date, date_add, date_format, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SyntheticData\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Указываем сколько строк надо сгенерировать\n",
    "numbers_row = 100\n",
    "\n",
    "# Генерация данных для колонок id, name, city, email, age, salary, registration_date\n",
    "data = [(i, f\"Names_{i}\", f\"Cityname_{i}\") for i in range(1, numbers_row + 1)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"city\"])\n",
    "\n",
    "df = df.withColumn(\"email\", concat(col(\"name\"), lit(\"@example.\"), when(col(\"id\") % 2 == 0, \"ru\").otherwise(\"com\"))) \\\n",
    "    .withColumn(\"age\", (rand() * 78 + 18).cast(\"int\")) \\\n",
    "    .withColumn(\"salary\", (rand() * 150000 + 19242).cast(\"int\")) \\\n",
    "    .withColumn(\"registration_date\", date_add(current_date(), - (rand() * (col(\"age\") - 18)).cast(\"int\") * 365))\n",
    "\n",
    "df.cache()  # Кеширование DataFrame для более быстрой обработки\n",
    "\n",
    "# Замена 5% данных на значение NULL\n",
    "columns = [\"name\", \"email\", \"city\", \"age\", \"salary\", \"registration_date\"]\n",
    "df = df.select(\"id\", *[when(rand() <= 0.05, None).otherwise(col(column)).alias(column) for column in columns])\n",
    "\n",
    "# Открываем словарь с алгритмом шифрования\n",
    "with open('/home/jovyan/work/PySpark_test/General_task_№2/utils/encryption_dict.json', 'r') as f:\n",
    "    encryption_dict = json.load(f)\n",
    "\n",
    "# Отправляем на узлы кластера\n",
    "broadcast_dict = sc.broadcast(encryption_dict).value\n",
    "\n",
    "# Функция для шифрования\n",
    "def encrypt_string(input_string):\n",
    "    if input_string is None:\n",
    "        return None\n",
    "    return ''.join(broadcast_dict.get(char, char) for char in str(input_string))\n",
    "\n",
    "# Определяем UDF для запуска функции шифрования\n",
    "encrypt_udf = udf(encrypt_string, StringType())\n",
    "\n",
    "# Применяем UDF к колоннам\n",
    "df = df.withColumn(\"name\", encrypt_udf(col(\"name\"))) \\\n",
    "    .withColumn(\"email\", encrypt_udf(col(\"email\"))) \\\n",
    "    .withColumn(\"city\", encrypt_udf(col(\"city\"))) \\\n",
    "    .withColumn(\"age\", encrypt_udf(col(\"age\"))) \\\n",
    "    .withColumn(\"salary\", encrypt_udf(col(\"salary\"))) \\\n",
    "    .withColumn(\"registration_date\", encrypt_udf(col(\"registration_date\")))\n",
    "\n",
    "# Пишу свое имя на первой строке колонки name\n",
    "df = df.withColumn(\"name\", when(col(\"id\") == 1, \"ATAGAEV\").otherwise(col(\"name\")))\n",
    "\n",
    "# Установка текущей даты и количества строк для имени файла\n",
    "df_date = spark.sql(\"SELECT current_date()\")\n",
    "current_date = df_date.select(date_format(\"current_date\", \"yyyy-MM-dd\")).first()[0]\n",
    "name_csv = encrypt_string(numbers_row)\n",
    "\n",
    "# Путь к сохранению CSV файла\n",
    "path = f\"/home/jovyan/work/PySpark_test/config/temp/{current_date}-{name_csv}.csv\"\n",
    "path_rename = f\"/home/jovyan/work/PySpark_test/config/{current_date}-{name_csv}.csv\"\n",
    "# Запись DataFrame в CSV\n",
    "df.coalesce(1).write.csv(path, header=True, mode=\"overwrite\")  \n",
    "\n",
    "# Переименование сгенерированного файла\n",
    "for file in os.listdir(path):\n",
    "    file_path = os.path.join(path, file)\n",
    "    if file.startswith(\"part-\"):\n",
    "        os.rename(file_path, path_rename)\n",
    "\n",
    "# Удаление временных файлов и папки\n",
    "for file in os.listdir(path):\n",
    "    file_path = os.path.join(path, file)\n",
    "    os.remove(file_path)\n",
    "    \n",
    "os.rmdir(path)\n",
    "\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0055006-61f6-44f4-a04c-1f0860b22664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+--------------------+-----------+---+------+-----------------+\n",
      "| id|    name|               email|       city|age|salary|registration_date|\n",
      "+---+--------+--------------------+-----------+---+------+-----------------+\n",
      "|  1| ATAGAEV| Names_1@example.com| Cityname_1| 80|134472|       2017-08-03|\n",
      "|  2| Names_2|  Names_2@example.ru| Cityname_2| 53|128621|       2010-08-05|\n",
      "|  3| Names_3| Names_3@example.com| Cityname_3| 69| 42612|       2020-08-02|\n",
      "|  4| Names_4|  Names_4@example.ru| Cityname_4| 62| 92485|       1982-08-12|\n",
      "|  5| Names_5| Names_5@example.com| Cityname_5| 85| 40284|       2002-08-07|\n",
      "|  6| Names_6|  Names_6@example.ru| Cityname_6| 24| 60038|       2020-08-02|\n",
      "|  7| Names_7| Names_7@example.com| Cityname_7| 42| 93294|       2006-08-06|\n",
      "|  8| Names_8|  Names_8@example.ru| Cityname_8| 80|119254|       2003-08-07|\n",
      "|  9| Names_9| Names_9@example.com| Cityname_9| 74| 32157|       1976-08-13|\n",
      "| 10|Names_10| Names_10@example.ru|Cityname_10| 78|157504|       2020-08-02|\n",
      "| 11|Names_11|                NULL|Cityname_11| 49|147955|       2016-08-03|\n",
      "| 12|Names_12| Names_12@example.ru|Cityname_12| 63| 71959|       1993-08-09|\n",
      "| 13|Names_13|Names_13@example.com|Cityname_13| 95|106356|             NULL|\n",
      "| 14|    NULL| Names_14@example.ru|Cityname_14| 63| 58484|       2023-08-02|\n",
      "| 15|Names_15|Names_15@example.com|Cityname_15| 59| 20368|       2003-08-07|\n",
      "| 16|Names_16| Names_16@example.ru|Cityname_16| 35| 38317|       2010-08-05|\n",
      "| 17|Names_17|                NULL|Cityname_17| 83|143380|       1966-08-16|\n",
      "| 18|Names_18|                NULL|Cityname_18| 79| 86425|       1999-08-08|\n",
      "| 19|Names_19|Names_19@example.com|Cityname_19| 32| 82921|       2018-08-03|\n",
      "| 20|Names_20| Names_20@example.ru|Cityname_20| 57|139479|       2005-08-06|\n",
      "+---+--------+--------------------+-----------+---+------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##########################################################################\n",
    "# Блок кода для дешифрования\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, current_date, date_format\n",
    "from pyspark.sql.types import StringType\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Инициализация Spark\n",
    "spark = SparkSession.builder.appName(\"SyntheticData\") \\\n",
    "    .config(\"spark.master\", \"local\") \\\n",
    "    .config('spark.driver.memory', '4g') \\\n",
    "    .config('spark.executor.cores', '4') \\\n",
    "    .config('spark.executor.memory', '8g') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Указываем сколько было строк в зашифрованном файле\n",
    "numbers_row = 100\n",
    "\n",
    "# Открытие словаря шифрования\n",
    "with open('/home/jovyan/work/PySpark_test/General_task_№2/utils/encryption_dict.json', 'r') as f:\n",
    "    encryption_dict = json.load(f)\n",
    "    \n",
    "# Отправляем на узлы кластера\n",
    "broadcast_dict = sc.broadcast(encryption_dict).value\n",
    "\n",
    "# Функция для шифрования\n",
    "def encrypt_string(input_string):\n",
    "    if input_string is None:\n",
    "        return None\n",
    "    return ''.join(broadcast_dict.get(char, char) for char in str(input_string))\n",
    "\n",
    "# Установка текущей даты и количества строк для имени файла\n",
    "df_date = spark.sql(\"SELECT current_date()\")\n",
    "current_date = df_date.select(date_format(\"current_date\", \"yyyy-MM-dd\")).first()[0]\n",
    "name_csv = encrypt_string(numbers_row)\n",
    "path_open = f\"/home/jovyan/work/PySpark_test/config/{current_date}-{name_csv}.csv\"\n",
    "\n",
    "# Чтение DataFrame из CSV\n",
    "df = spark.read.csv(path_open, header=True)\n",
    "\n",
    "# Создание обратного словаря\n",
    "decryption_dict = {v: k for k, v in broadcast_dict.items()}\n",
    "\n",
    "# Функция для дешифрования\n",
    "def decrypt_string(encrypted):\n",
    "    if encrypted is None:\n",
    "        return None \n",
    "    decrypted = []\n",
    "    pattern = '|'.join(re.escape(key) for key in decryption_dict.keys())\n",
    "    matches = re.findall(pattern, encrypted)\n",
    "    if not matches:\n",
    "        return encrypted\n",
    "    for match in matches:\n",
    "        decrypted.append(decryption_dict.get(match, match))\n",
    "    return ''.join(decrypted)\n",
    "\n",
    "# Определяем UDF для дешифрования\n",
    "decrypt_udf = udf(decrypt_string, StringType())\n",
    "\n",
    "# Применяем UDF к нужным колоннам\n",
    "df = df.withColumn(\"name\", decrypt_udf(col(\"name\"))) \\\n",
    "    .withColumn(\"email\", decrypt_udf(col(\"email\"))) \\\n",
    "    .withColumn(\"city\", decrypt_udf(col(\"city\"))) \\\n",
    "    .withColumn(\"age\", decrypt_udf(col(\"age\"))) \\\n",
    "    .withColumn(\"salary\", decrypt_udf(col(\"salary\"))) \\\n",
    "    .withColumn(\"registration_date\", decrypt_udf(col(\"registration_date\")))\n",
    "\n",
    "# Путь к сохранению CSV файла\n",
    "path = f\"/home/jovyan/work/PySpark_test/config/temp/{current_date}-dev.csv\"\n",
    "path_rename = f\"/home/jovyan/work/PySpark_test/config/{current_date}-dev.csv\"\n",
    "# Запись DataFrame в CSV\n",
    "df.coalesce(1).write.csv(path, header=True, mode=\"overwrite\")  \n",
    "\n",
    "# Переименование сгенерированного файла\n",
    "for file in os.listdir(path):\n",
    "    file_path = os.path.join(path, file)\n",
    "    if file.startswith(\"part-\"):\n",
    "        os.rename(file_path, path_rename)\n",
    "\n",
    "# Удаление временных файлов и папки\n",
    "for file in os.listdir(path):\n",
    "    file_path = os.path.join(path, file)\n",
    "    os.remove(file_path)\n",
    "    \n",
    "os.rmdir(path)\n",
    "\n",
    "df.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1e4c7-15d3-4232-bbfc-a3ad900cf5a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
