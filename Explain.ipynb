{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ac673d4-37c5-4fbe-9123-2f4c544f82b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/29 14:44:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Name ASC NULLS FIRST], true\n",
      "+- Project [Name#0, Age#1L]\n",
      "   +- Filter (Age#1L > cast(30 as bigint))\n",
      "      +- LogicalRDD [Name#0, Age#1L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Name: string, Age: bigint\n",
      "Sort [Name#0 ASC NULLS FIRST], true\n",
      "+- Project [Name#0, Age#1L]\n",
      "   +- Filter (Age#1L > cast(30 as bigint))\n",
      "      +- LogicalRDD [Name#0, Age#1L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Name#0 ASC NULLS FIRST], true\n",
      "+- Filter (isnotnull(Age#1L) AND (Age#1L > 30))\n",
      "   +- LogicalRDD [Name#0, Age#1L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Name#0 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Name#0 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=12]\n",
      "      +- Filter (isnotnull(Age#1L) AND (Age#1L > 30))\n",
      "         +- Scan ExistingRDD[Name#0,Age#1L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Catalyst Optimizer Example\").getOrCreate()\n",
    "\n",
    "\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "\n",
    "\n",
    "result = df.filter(df.Age > 30).select(\"Name\", \"Age\").orderBy(\"Name\")\n",
    "\n",
    "# Показываем логический и физический планы\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa859b3f-bd03-49c2-8bb5-169fa30b7ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f8268d-7881-4549-8fcf-214e89b751a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/07/29 18:55:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['mod3 ASC NULLS FIRST], true\n",
      "+- Aggregate [mod3#2L], [mod3#2L, sum(id#0L) AS sum(id)#13L]\n",
      "   +- Filter (mod2#3L = cast(1 as bigint))\n",
      "      +- LogicalRDD [id#0L, name#1, mod3#2L, mod2#3L], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "mod3: bigint, sum(id): bigint\n",
      "Sort [mod3#2L ASC NULLS FIRST], true\n",
      "+- Aggregate [mod3#2L], [mod3#2L, sum(id#0L) AS sum(id)#13L]\n",
      "   +- Filter (mod2#3L = cast(1 as bigint))\n",
      "      +- LogicalRDD [id#0L, name#1, mod3#2L, mod2#3L], false\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [mod3#2L ASC NULLS FIRST], true\n",
      "+- Aggregate [mod3#2L], [mod3#2L, sum(id#0L) AS sum(id)#13L]\n",
      "   +- Project [id#0L, mod3#2L]\n",
      "      +- Filter (isnotnull(mod2#3L) AND (mod2#3L = 1))\n",
      "         +- LogicalRDD [id#0L, name#1, mod3#2L, mod2#3L], false\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [mod3#2L ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(mod3#2L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=26]\n",
      "      +- HashAggregate(keys=[mod3#2L], functions=[sum(id#0L)], output=[mod3#2L, sum(id)#13L])\n",
      "         +- Exchange hashpartitioning(mod3#2L, 200), ENSURE_REQUIREMENTS, [plan_id=23]\n",
      "            +- HashAggregate(keys=[mod3#2L], functions=[partial_sum(id#0L)], output=[mod3#2L, sum#17L])\n",
      "               +- Project [id#0L, mod3#2L]\n",
      "                  +- Filter (isnotnull(mod2#3L) AND (mod2#3L = 1))\n",
      "                     +- Scan ExistingRDD[id#0L,name#1,mod3#2L,mod2#3L]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Tungsten Example\") \\\n",
    "    .config(\"spark.sql.codegen.wholeStage\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "data = [(i, f\"Name_{i % 5}\", i % 3, i % 2) for i in range(1, 1000001)]\n",
    "df = spark.createDataFrame(data, [\"id\", \"name\", \"mod3\", \"mod2\"])\n",
    "\n",
    "# Применяем фильтр и агрегацию, чтобы увидеть работу Tungsten\n",
    "result = df.filter(col(\"mod2\") == 1) \\\n",
    "           .groupBy(\"mod3\") \\\n",
    "           .agg({\"id\": \"sum\"}) \\\n",
    "           .orderBy(\"mod3\")\n",
    "\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b2b72d3-b46f-4109-99ae-41c76b132f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f23b2-9e19-46fd-8b2f-6a03b8564954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
